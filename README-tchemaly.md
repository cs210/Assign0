## Hi!üëãüòÑ I'm Trishia El Chemaly

I'm an engineer, XR developer, and researcher, currently building XR technology that redefines surgery. My skill set spans from research techniques to the development of software and hardware in XR, computer vision, and HCI.  As an interdisciplinary PhD candidate at Stanford, I am fortunate to collaborate with multiple groups including the Surgical Simulation lab, the Incubator for Medical Mixed and Extended Reality at Stanford, the Visualization Lab, the Body Magnetic Resonance Group, and the Salisbury Robotics Lab. 

Previously, I was the president of [Stanford XR](https://www.stanfordxr.org/about), Stanford University‚Äôs student organization for extended reality. 

![DSC_0480 - Copy](https://github.com/user-attachments/assets/db255a87-0dc3-4daa-9f39-ce20efe6c178)

---

## üëìPast Experiences
**Student Researcher @ Google**
- Worked alongside the Google AR team that recently launched [Android XR](https://blog.google/products/android/android-xr/)
- Led the research on visual comfort and motion sickness for Moohan, Samsung's first headset powered by Android XR
- Designed and established technical and user benchmarks for comfort in AR passthrough

**Graduate Research Assistant @ IMMERS (Incubator for Medical Mixed and Extended Reality at Stanford)**
- Developed AR solutions for medical needs identified while working with clinicians
- Implemented stereoscopic calibration for AR visualization in microscopic surgery
- Trained and fine-tuned computer vision models for tracking in surgical AR
- Co-authored a CHI paper on interactive shape sonification for tumor localization in breast cancer surgery
- Mentored undergraduate students developing applications for diverse AR devices

**RAD206: Mixed-Reality in Medicine Teaching Assistant @ Stanford University**
- Created and graded AR assignments in Unity and C#
- Mentored groups of students working on class projects with Microsoft HoloLens 2

**CS 12SI: Spatial Computing Workshop Student Lecturer @ Stanford University**
- Co-created and co-led CS12SI, a course listed under Stanford's CS department, with fellow Stanford XR members
- Taught Stanford students skills to create AR applications for Apple Vision Pro, both device and simulator
  
**Project Experiences from Stanford Courses**
- CS 231N (CNNs for Visual Recignition): Brand Product Identification with Deep Learning
- CS 231A (Computer Vision: From 3D Reconstruction to Recognition): Automatic Tracking for Anatomical Landmarks 
- CS 224N (NLP with Deep Learning): Context-Aware Gesture Interpretation in AR/VR
- EE 267 (Virtual Reality): AR in Head and Neck Surgery
- Design 374 (Creativity in Research): ResQVision - AR Aid for Emergency Responders
- Design 284 (Designing for XR): Streamlining 3D Data Processing for ML

**StanfordXR**
where I led several initiatives such as founding [Immerse The Bay](https://immersethebay.stanfordxr.org/), Stanford's leading XR hackathon. 

---

## üìöSkills
**AR/VR Development** 
- Unity, C#, visionOS, AR Foundation, RealityKit, OpenXR, Blender
- Apple Vision Pro, Meta Quest 3 and Quest Pro, Microsoft HoloLens, Magic Leap

**CS**
- Computer Vision and Deep Learning: Python, MATLAB, Unity and C#
- Object-oriented Programming: C#, Python, JavaScript
- Robotics programming: C/C++, RobotC, ROS
- Signal and Image Processing, Computational Modeling: Python, MATLAB

---

## ü§ùConnect with me
- [LinkedIn](https://www.linkedin.com/in/trishia-chemaly/)
- [Portfolio](https://tchemaly.github.io/)
